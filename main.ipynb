{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP + SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from segment_anything import build_sam, SamAutomaticMaskGenerator\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Download the model weights to load them here\n",
    "sam = build_sam(checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "sam = sam.to(device)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"assets/example-image.jpg\"\n",
    "image_path = \"assets/new_GD_DOM_RGB_LONGLAT.png\"\n",
    "image_path = \"segmentResult/67.png\"\n",
    "image_path = \"isaid_segm/val/images/images/P0003.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(type(image))\n",
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_box_xywh_to_xyxy(box):\n",
    "    x1 = box[0]\n",
    "    y1 = box[1]\n",
    "    x2 = box[0] + box[2]\n",
    "    y2 = box[1] + box[3]\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image, segmentation_mask):\n",
    "    image_array = np.array(image)\n",
    "    segmented_image_array = np.zeros_like(image_array)\n",
    "    segmented_image_array[segmentation_mask] = image_array[segmentation_mask]\n",
    "    segmented_image = Image.fromarray(segmented_image_array)\n",
    "    black_image = Image.new(\"RGB\", image.size, (0, 0, 0))\n",
    "    transparency_mask = np.zeros_like(segmentation_mask, dtype=np.uint8)\n",
    "    transparency_mask[segmentation_mask] = 255\n",
    "    transparency_mask_image = Image.fromarray(transparency_mask, mode='L')\n",
    "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
    "    return black_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out all masks\n",
    "image = Image.open(image_path)\n",
    "print(np.array(image).shape)\n",
    "print(np.array(image).dtype)\n",
    "display(image)\n",
    "cropped_boxes = []\n",
    "\n",
    "for mask in masks:\n",
    "    cropped_boxes.append(segment_image(image, mask[\"segmentation\"]).crop(convert_box_xywh_to_xyxy(mask[\"bbox\"])))\n",
    "\n",
    "# for box in cropped_boxes:\n",
    "    # display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def retriev(elements: list[Image.Image], search_text: list[str]) -> int:\n",
    "    preprocessed_images = [preprocess(image).to(device) for image in elements]\n",
    "    tokenized_text = clip.tokenize(search_text).to(device)\n",
    "    stacked_images = torch.stack(preprocessed_images)\n",
    "    image_features = model.encode_image(stacked_images)\n",
    "    text_features = model.encode_text(tokenized_text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    probs = 100. * image_features @ text_features.T\n",
    "    # print(probs[:,:])\n",
    "    # print(probs[:, :].softmax(dim=0))\n",
    "    return probs[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_of_values_above_threshold(values, threshold):\n",
    "    for i, v in enumerate(values):\n",
    "        # print(i, v)\n",
    "        # print(v > 20)\n",
    "        if (v>20):\n",
    "            print(i, v)\n",
    "    return [i for i, v in enumerate(values) if v > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MAP = dict({\n",
    "    # 'background': (0, 0, 0),\n",
    "    'ship': (0, 0, 63),\n",
    "    'storage tank': (0, 191, 127),\n",
    "    'baseball diamond': (0, 63, 0),\n",
    "    'tennis court': (0, 63, 127),\n",
    "    'basketball court': (0, 63, 191),\n",
    "    'ground track field': (0, 63, 255),\n",
    "    'bridge': (0, 127, 63),\n",
    "    'large vehicle': (0, 127, 127),\n",
    "    'small vehicle': (0, 0, 127),\n",
    "    'helicopter': (0, 0, 191),\n",
    "    'swimming pool': (0, 0, 255),\n",
    "    'roundabout': (0, 63, 63),\n",
    "    'soccer ball field': (0, 127, 191),\n",
    "    'plane': (0, 127, 255),\n",
    "    'harbor': (0, 100, 155),\n",
    "})\n",
    "# background,ship,storage tank,baseball diamond,tennis court,basketball court,ground track field,bridge,large vehicle,small vehicle,helicopter,swimming pool,roundabout,soccer ball field,plane,harbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list = []\n",
    "color_list = []\n",
    "hh_list = []\n",
    "opacity = 255\n",
    "\n",
    "for k, v in COLOR_MAP.items():\n",
    "    text_list.append(\"a photo of a {}\".format(k))\n",
    "    hh_list.append(k)\n",
    "    color_list.append((v[0], v[1], v[2], opacity))\n",
    "\n",
    "print(hh_list)\n",
    "for hh in hh_list:\n",
    "    print(hh, end=',')\n",
    "print('')\n",
    "print(text_list)\n",
    "print(color_list)\n",
    "\n",
    "scores = retriev(cropped_boxes, text_list)\n",
    "indices = range(0, len(scores))\n",
    "color_indices = scores.argmax(dim=1)\n",
    "\n",
    "segmentation_masks = []\n",
    "\n",
    "original_image = Image.open(image_path)\n",
    "overlay_image = Image.new('RGBA', image.size, (0, 0, 0, 0))\n",
    "# overlay_color = (255, 0, 0, 100)\n",
    "\n",
    "draw = ImageDraw.Draw(overlay_image)\n",
    "for i in indices:\n",
    "    draw.bitmap((0, 0), \n",
    "                Image.fromarray(masks[i][\"segmentation\"].astype('uint8') * 255), \n",
    "                fill=color_list[scores[i].argmax(dim=0)])\n",
    "\n",
    "print(scores[50].softmax(0))\n",
    "\n",
    "result_image = Image.alpha_composite(original_image.convert('RGBA'), overlay_image)\n",
    "display(result_image)\n",
    "result_image.save('P003_ans.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "import os\n",
    "for box in cropped_boxes:\n",
    "    # display(box)dd\n",
    "    cnt += 1\n",
    "    img_path = os.path.join('./segmentResult3', str(cnt)+'.png')\n",
    "    box.save(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mask in masks:\n",
    "    print((mask[\"segmentation\"].shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP-SAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
